{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parse_model_config(path)分析yolov3.cfg网络搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'angle': '0',\n",
       "  'batch': '16',\n",
       "  'burn_in': '1000',\n",
       "  'channels': '3',\n",
       "  'decay': '0.0005',\n",
       "  'exposure': '1.5',\n",
       "  'height': '416',\n",
       "  'hue': '.1',\n",
       "  'learning_rate': '0.001',\n",
       "  'max_batches': '500200',\n",
       "  'momentum': '0.9',\n",
       "  'policy': 'steps',\n",
       "  'saturation': '1.5',\n",
       "  'scales': '.1,.1',\n",
       "  'steps': '400000,450000',\n",
       "  'subdivisions': '1',\n",
       "  'type': 'net',\n",
       "  'width': '416'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '32',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '64',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '2',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '32',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '64',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear', 'from': '-3', 'type': 'shortcut'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '2',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '64',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear', 'from': '-3', 'type': 'shortcut'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '64',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear', 'from': '-3', 'type': 'shortcut'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '2',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear', 'from': '-3', 'type': 'shortcut'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear', 'from': '-3', 'type': 'shortcut'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear', 'from': '-3', 'type': 'shortcut'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear', 'from': '-3', 'type': 'shortcut'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear', 'from': '-3', 'type': 'shortcut'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear', 'from': '-3', 'type': 'shortcut'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear', 'from': '-3', 'type': 'shortcut'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear', 'from': '-3', 'type': 'shortcut'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '2',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear', 'from': '-3', 'type': 'shortcut'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear', 'from': '-3', 'type': 'shortcut'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear', 'from': '-3', 'type': 'shortcut'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear', 'from': '-3', 'type': 'shortcut'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear', 'from': '-3', 'type': 'shortcut'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear', 'from': '-3', 'type': 'shortcut'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear', 'from': '-3', 'type': 'shortcut'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear', 'from': '-3', 'type': 'shortcut'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '1024',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '2',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '1024',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear', 'from': '-3', 'type': 'shortcut'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '1024',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear', 'from': '-3', 'type': 'shortcut'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '1024',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear', 'from': '-3', 'type': 'shortcut'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '1024',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear', 'from': '-3', 'type': 'shortcut'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '1024',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '1024',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '1024',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear',\n",
       "  'batch_normalize': 0,\n",
       "  'filters': '255',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'anchors': '10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326',\n",
       "  'classes': '80',\n",
       "  'ignore_thresh': '.7',\n",
       "  'jitter': '.3',\n",
       "  'mask': '6,7,8',\n",
       "  'num': '9',\n",
       "  'random': '1',\n",
       "  'truth_thresh': '1',\n",
       "  'type': 'yolo'},\n",
       " {'layers': '-4', 'type': 'route'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'stride': '2', 'type': 'upsample'},\n",
       " {'layers': '-1, 61', 'type': 'route'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '512',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear',\n",
       "  'batch_normalize': 0,\n",
       "  'filters': '255',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'anchors': '10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326',\n",
       "  'classes': '80',\n",
       "  'ignore_thresh': '.7',\n",
       "  'jitter': '.3',\n",
       "  'mask': '3,4,5',\n",
       "  'num': '9',\n",
       "  'random': '1',\n",
       "  'truth_thresh': '1',\n",
       "  'type': 'yolo'},\n",
       " {'layers': '-4', 'type': 'route'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'stride': '2', 'type': 'upsample'},\n",
       " {'layers': '-1, 36', 'type': 'route'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '128',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'leaky',\n",
       "  'batch_normalize': '1',\n",
       "  'filters': '256',\n",
       "  'pad': '1',\n",
       "  'size': '3',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'activation': 'linear',\n",
       "  'batch_normalize': 0,\n",
       "  'filters': '255',\n",
       "  'pad': '1',\n",
       "  'size': '1',\n",
       "  'stride': '1',\n",
       "  'type': 'convolutional'},\n",
       " {'anchors': '10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326',\n",
       "  'classes': '80',\n",
       "  'ignore_thresh': '.7',\n",
       "  'jitter': '.3',\n",
       "  'mask': '0,1,2',\n",
       "  'num': '9',\n",
       "  'random': '1',\n",
       "  'truth_thresh': '1',\n",
       "  'type': 'yolo'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "def parse_model_config(path):\n",
    "    \"\"\"Parses the yolo-v3 layer configuration file and returns module definitions\"\"\"\n",
    "    file = open(path, 'r')\n",
    "    lines = file.read().split('\\n')\n",
    "    lines = [x for x in lines if x and not x.startswith('#')]\n",
    "    lines = [x.rstrip().lstrip() for x in lines] # get rid of fringe whitespaces\n",
    "    module_defs = []\n",
    "    for line in lines:\n",
    "        if line.startswith('['): # This marks the start of a new block\n",
    "            module_defs.append({})\n",
    "            module_defs[-1]['type'] = line[1:-1].rstrip()\n",
    "            if module_defs[-1]['type'] == 'convolutional':\n",
    "                module_defs[-1]['batch_normalize'] = 0\n",
    "        else:\n",
    "            key, value = line.split(\"=\")\n",
    "            value = value.strip()\n",
    "            module_defs[-1][key.rstrip()] = value.strip()\n",
    "\n",
    "    return module_defs\n",
    "path=r'/py/jupyter-notebook/yolov3-master/cfg/yolov3.cfg'\n",
    "print(parse_model_config(path)[0]['height'])    #列表嵌套字典\n",
    "parse_model_config(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# route层的分析"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "这个链接解释的不错：https://www.cnblogs.com/makefile/p/YOLOv3.html  \n",
    "其实就是conta连接层，位置是放在上采样层的后面，所以直接在序贯模型中搭建，然后融合上采样层和前面层的特征。\n",
    "\n",
    "参数解释：\n",
    "其cfg中的参数就是对应层的索引：只有一个参数时，返回它输出这一层通过该值索引的特征图（实验中设置为了-4，所以层级将输出路由层之前第三个层的特征图）；当层级有两个值时，它将返回由这两个值索引的拼接特征图。如配置文件中为-1 和 61，因此该层级将输出从前一层级（-1）到第 61 层的特征图，并将它们按深度拼接。\n",
    "\n",
    "为什么是-4？\n",
    "因为还有一个yolo层。注意，网上的图不全，通过后面我的模型输出尺度打印可以看出，其实route层放在yolo层之后，也就是结束了一个尺度检测后，才开始route的，那么由于当前层未被append到存储列表，所以-1表示的上一层是yolo层，以13*13尺度为例，-1层是（1,13*13*3,85），-2层是（1,255,13,13），-3是个卷积(1,1024,13,13),-4才是我们需要的(1,512,13,13)。对其卷积一次，就可以上采样融合之类的继续序贯模型走下去（结合网络结构图和模型输出分析）。\n",
    "\n",
    "分析：filters = sum([output_filters[i + 1 if i > 0 else i] for i in layers])，其中layers是cfg文件中的route层的数字（四组：[-4],[-1,61],[-4],[-1,36]）。当i<0时，输出output_filters[i]，i=-4输出的是倒数第四个层的卷积核数（该层通道数），i=-1输出的就是上一层了；i>0输出的就是output_filters[i+1]（cfg配置已经给了）\n",
    "\n",
    "至于单个的route层为什么存在？\n",
    "因为实际上网络搭建的是一个序贯模型，到了头，就回不来了，所以第一张特征图13*13搭建完了后，进行YOLO层检测，第一个维度就完事了。但是想要回头不存在的，只能通过toute取出检测层网上三层（自身算最后一个，所以-4是往上数三个），在这个基础上，继续再往下进行。所以我们看到的三个分支结构，通过四个route实际上罗列成了序贯模型！下面forward传播的尺度变化可以参照来看！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shortcut层的分析"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "借鉴ResNet思想回归残差便于收敛，这里是直接的加和，不像route是通道叠加\n",
    "看代码：filters = output_filters[int(module_def['from'])]\n",
    "from全都是-3，所以是取当前层往上数三个便是（当前不是-1！当前层还没被append不在列表中）。残差模块是三个一组：conv1+conv2+sum这个sum就是shortcut了，那么往上数三个，就是上一层的sum，所以这层的sum和上层的sum(shortcut层)通道数一样，从而确定下面的shortcut层卷积核数（必然一样，不然没法加）。至于不能加的通道数突变层的地方....不用残差模块不就完了，通道变完了（如512->1024）,继续对1024维的ResNet。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# anchors extraction（yolo layer）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchor_idxs:[0, 1, 2]\n",
      "\n",
      "anchors:[10.0, 13.0, 16.0, 30.0, 33.0, 23.0, 30.0, 61.0, 62.0, 45.0, 59.0, 119.0, 116.0, 90.0, 156.0, 198.0, 373.0, 326.0]\n",
      "\n",
      "anchors:[(10.0, 13.0), (16.0, 30.0), (33.0, 23.0), (30.0, 61.0), (62.0, 45.0), (59.0, 119.0), (116.0, 90.0), (156.0, 198.0), (373.0, 326.0)]\n",
      "\n",
      "anchors:[(10.0, 13.0), (16.0, 30.0), (33.0, 23.0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# module_def['mask']='0,1,2'为例\n",
    "anchor_idxs = [int(x) for x in '0,1,2'.split(',')] \n",
    "print('anchor_idxs:{}\\n'.format(anchor_idxs))\n",
    "\n",
    "# module_def['anchors']='10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326'\n",
    "anchors = [float(x) for x in '10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326'.split(',')]\n",
    "print('anchors:{}\\n'.format(anchors))\n",
    "\n",
    "#按照宽高组合成9个先验框anchors\n",
    "anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]\n",
    "print('anchors:{}\\n'.format(anchors))\n",
    "\n",
    "#三个yolo层，每个yolo预测一个尺度，每个尺度使用与尺度匹配的3个anchor作为先验框（如这里mask为012选取最小的三个anchor回归最大的特征图52*52，其他的依次如此）\n",
    "anchors = [anchors[i] for i in anchor_idxs]\n",
    "print('anchors:{}\\n'.format(anchors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "'''\n",
    "首先给特征图编号，三个尺寸相似，以13*13特征图8为例分析\n",
    "anchors:[(10.0, 13.0), (16.0, 30.0), (33.0, 23.0)]\n",
    "stride:32(grid cell size)\n",
    "'''\n",
    "class YOLOLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, anchors, nC, img_dim, anchor_idxs, cfg):\n",
    "        super(YOLOLayer, self).__init__()\n",
    "\n",
    "        anchors = [(a_w, a_h) for a_w, a_h in anchors]  # (pixels)\n",
    "        nA = len(anchors)\n",
    "\n",
    "        self.anchors = anchors\n",
    "        self.nA = nA  # number of anchors (3)\n",
    "        self.nC = nC  # number of classes (80)\n",
    "        self.bbox_attrs = 5 + nC\n",
    "        self.img_dim = img_dim  # from hyperparams in cfg file, NOT from parser\n",
    "\n",
    "        if anchor_idxs[0] == (nA * 2):  # 6\n",
    "            stride = 32\n",
    "        elif anchor_idxs[0] == nA:  # 3\n",
    "            stride = 16\n",
    "        else:\n",
    "            stride = 8\n",
    "\n",
    "        if cfg.endswith('yolov3-tiny.cfg'):\n",
    "            stride *= 2\n",
    "\n",
    "        # Build anchor grids\n",
    "        nG = int(self.img_dim / stride)  # number grid points,如nG=13为特征图尺寸\n",
    "        self.grid_x = torch.arange(nG).repeat(nG, 1).view([1, 1, nG, nG]).float()  '''torch.Size([1, 1, 13, 13])'''\n",
    "        self.grid_y = torch.arange(nG).repeat(nG, 1).t().view([1, 1, nG, nG]).float()\n",
    "        self.scaled_anchors = torch.FloatTensor([(a_w / stride, a_h / stride) for a_w, a_h in anchors])#stride=32降采样步长 \n",
    "        self.anchor_w = self.scaled_anchors[:, 0:1].view((1, nA, 1, 1))  #nA=3，anchor数目\n",
    "        self.anchor_h = self.scaled_anchors[:, 1:2].view((1, nA, 1, 1))\n",
    "        '''\n",
    "        anchor_w的shape：torch.Size([1, 3, 1, 1])，内容为：\n",
    "        tensor([[[[0.3125]],\n",
    "                 [[0.5000]],\n",
    "                 [[1.0312]]]])\n",
    "        按照降采样步长（原图每个grid cell的尺寸），自己乘着试试看，如0.3125*32=10（第一个anchor的w）\n",
    "        (atten)可见，anchor是经过降采样步长缩放的，而不是416图片尺寸\n",
    "        '''\n",
    "        self.weights = class_weights()                                  #统计了coco的80类gt出现的归一化比重作为权值\n",
    "\n",
    "        self.loss_means = torch.ones(6)                                 #6维行向量，用1初始化\n",
    "        self.tx, self.ty, self.tw, self.th = [], [], [], []             \n",
    "        self.yolo_layer = anchor_idxs[0] / nA                           # 2, 1, 0\n",
    "\n",
    "    def forward(self, p, targets=None, batch_report=False, var=None):\n",
    "        FT = torch.cuda.FloatTensor if p.is_cuda else torch.FloatTensor\n",
    "        '''p为torch.Size([1, 255, 13, 13])，1是batch_size'''\n",
    "        bs = p.shape[0]  '''batch size'''\n",
    "        nG = p.shape[2]  # number of grid points\n",
    "\n",
    "        if p.is_cuda and not self.grid_x.is_cuda:   #将所有参数都放到cuda上\n",
    "            self.grid_x, self.grid_y = self.grid_x.cuda(), self.grid_y.cuda()\n",
    "            self.anchor_w, self.anchor_h = self.anchor_w.cuda(), self.anchor_h.cuda()\n",
    "            self.weights, self.loss_means = self.weights.cuda(), self.loss_means.cuda()\n",
    "\n",
    "    \n",
    "        p = p.view(bs, self.nA, self.bbox_attrs, nG, nG).permute(0, 1, 3, 4, 2).contiguous()  # prediction\n",
    "        '''\n",
    "        把255的列向量信息，加一个维度分开成3个box\n",
    "        p的shape为：torch.Size([1, 255, 13, 13])-->torch.Size([1, 3, 13, 13, 85]) ，(bs, anchors, grid, grid, xywh+conf+class)\n",
    "        注意，当前YOLO层分配了3个anchor,每个grid cell按照anchor尺寸预测三个box，上面是以13*13尺寸为例的\n",
    "        '''\n",
    "        \n",
    "        # Training\n",
    "        if targets is not None:\n",
    "            MSELoss = nn.MSELoss()                      #实例化均方损失计算器，后面调用这个变量计算损失\n",
    "            BCEWithLogitsLoss = nn.BCEWithLogitsLoss()  #交叉熵+sigmoid\n",
    "            CrossEntropyLoss = nn.CrossEntropyLoss()    #多分类交叉熵\n",
    "\n",
    "            # Get outputs\n",
    "            #训练前向传入这里的x,y用sigmoid归一化（这里的xy是相对于当前位置的，转化为真实xy还需加上该grid cell的位置）\n",
    "            x = torch.sigmoid(p[..., 0])  \n",
    "            y = torch.sigmoid(p[..., 1])  \n",
    "            p_conf = p[..., 4]            \n",
    "            p_cls = p[..., 5:]            \n",
    "            '''\n",
    "            为什么是这个尺寸？13*13个cell，每个cell有3个anchor回归box，所有=以有3*13*13套坐标\n",
    "            Center x,y：torch.Size([1, 3, 13, 13])   Conf：torch.Size([1, 3, 13, 13])\n",
    "            Class：torch.Size([1, 3, 13, 13, 80])\n",
    "            '''\n",
    "\n",
    "            # Width and height (yolo method)\n",
    "            w = p[..., 2] \n",
    "            h = p[..., 3] \n",
    "            width = torch.exp(w.data) * self.anchor_w   '''计算传播注意调用data方法提取张量（除data里面还有grad_fn，用于反向传播的梯度）'''\n",
    "            height = torch.exp(h.data) * self.anchor_h\n",
    "            '''\n",
    "            w,h: torch.Size([1, 3, 13, 13]) 理由同上\n",
    "            width，height： 计算的是预测box的真实wh（和论文说的一样，取e指数，乘anchor）\n",
    "            '''\n",
    "\n",
    "            # Width and height (power method)\n",
    "            # w = torch.sigmoid(p[..., 2])  # Width\n",
    "            # h = torch.sigmoid(p[..., 3])  # Height\n",
    "            # width = ((w.data * 2) ** 2) * self.anchor_w\n",
    "            # height = ((h.data * 2) ** 2) * self.anchor_h\n",
    "\n",
    "            p_boxes = None\n",
    "            if batch_report:    #计算mAP。\n",
    "                # Predicted boxes: add offset and scale with anchors (in grid space, i.e. 0-13)\n",
    "                gx = x.data + self.grid_x[:, :, :nG, :nG]\n",
    "                gy = y.data + self.grid_y[:, :, :nG, :nG]\n",
    "                p_boxes = torch.stack((gx - width / 2,\n",
    "                                       gy - height / 2,\n",
    "                                       gx + width / 2,\n",
    "                                       gy + height / 2), 4)  # x1y1x2y2\n",
    "                '''\n",
    "                （1）x.data: torch.Size([1, 3, 13, 13])      self.grid_x[:, :, :nG, :nG]: torch.Size([1, 1, 13, 13])\n",
    "                    gx: torch.Size([1, 3, 13, 13])\n",
    "                    (atten)numpy和tensor中，如果数组的维度不一样，则默认在缺省的维度上全部相加，（列表不行）如：\n",
    "                        a=np.array(([1,1],[2,2]))；b=np.array(([1,1]))；a+b=array([[2, 2],[3, 3]])\n",
    "                    那么这里的意思是将grid point偏移加在预测的box坐标上。具体怎么加的？ \n",
    "                （2）grid_y[:, :, :nG, :nG]在后两维是13*13的矩阵，每维都是0-12,所以是将每个grid cell的位置编号加上去了，正好就是特征图的偏移\n",
    "                '''\n",
    "\n",
    "            tx, ty, tw, th, mask, tcls, TP, FP, FN, TC = \\\n",
    "                build_targets(p_boxes, p_conf, p_cls, targets, self.scaled_anchors, self.nA, self.nC, nG, batch_report)\n",
    "\n",
    "            tcls = tcls[mask]\n",
    "            if x.is_cuda:\n",
    "                tx, ty, tw, th, mask, tcls = tx.cuda(), ty.cuda(), tw.cuda(), th.cuda(), mask.cuda(), tcls.cuda()\n",
    "\n",
    "            # Compute losses\n",
    "            nT = sum([len(x) for x in targets])  # number of targets 总的gt的数目\n",
    "            nM = mask.sum().float()  # number of anchors (assigned to targets)\n",
    "            nB = len(targets)  # batch size\n",
    "            k = nM / nB\n",
    "            if nM > 0:\n",
    "                lx = k * MSELoss(x[mask], tx[mask])\n",
    "                ly = k * MSELoss(y[mask], ty[mask])\n",
    "                lw = k * MSELoss(w[mask], tw[mask])\n",
    "                lh = k * MSELoss(h[mask], th[mask])\n",
    "\n",
    "                # self.tx.extend(tx[mask].data.numpy())\n",
    "                # self.ty.extend(ty[mask].data.numpy())\n",
    "                # self.tw.extend(tw[mask].data.numpy())\n",
    "                # self.th.extend(th[mask].data.numpy())\n",
    "                # print([np.mean(self.tx), np.std(self.tx)],[np.mean(self.ty), np.std(self.ty)],[np.mean(self.tw), np.std(self.tw)],[np.mean(self.th), np.std(self.th)])\n",
    "                # [0.5040668, 0.2885492] [0.51384246, 0.28328574] [-0.4754091, 0.57951087] [-0.25998235, 0.44858757]\n",
    "                # [0.50184494, 0.2858976] [0.51747805, 0.2896323] [0.12962963, 0.6263085] [-0.2722081, 0.61574113]\n",
    "                # [0.5032071, 0.28825334] [0.5063132, 0.2808862] [0.21124361, 0.44760725] [0.35445485, 0.6427766]\n",
    "                # import matplotlib.pyplot as plt\n",
    "                # plt.hist(self.x)\n",
    "\n",
    "                # lconf = k * BCEWithLogitsLoss(p_conf[mask], mask[mask].float())\n",
    "\n",
    "                lcls = (k / 4) * CrossEntropyLoss(p_cls[mask], torch.argmax(tcls, 1))\n",
    "                # lcls = (k * 10) * BCEWithLogitsLoss(p_cls[mask], tcls.float())\n",
    "            else:\n",
    "                lx, ly, lw, lh, lcls, lconf = FT([0]), FT([0]), FT([0]), FT([0]), FT([0]), FT([0])\n",
    "\n",
    "            # lconf += k * BCEWithLogitsLoss(p_conf[~mask], mask[~mask].float())\n",
    "            lconf = (k * 64) * BCEWithLogitsLoss(p_conf, mask.float())\n",
    "\n",
    "            # Sum loss components\n",
    "            balance_losses_flag = False\n",
    "            if balance_losses_flag:\n",
    "                k = 1 / self.loss_means.clone()\n",
    "                loss = (lx * k[0] + ly * k[1] + lw * k[2] + lh * k[3] + lconf * k[4] + lcls * k[5]) / k.mean()\n",
    "\n",
    "                self.loss_means = self.loss_means * 0.99 + \\\n",
    "                                  FT([lx.data, ly.data, lw.data, lh.data, lconf.data, lcls.data]) * 0.01\n",
    "            else:\n",
    "                loss = lx + ly + lw + lh + lconf + lcls\n",
    "\n",
    "            # Sum False Positives from unassigned anchors\n",
    "            FPe = torch.zeros(self.nC)\n",
    "            if batch_report:\n",
    "                i = torch.sigmoid(p_conf[~mask]) > 0.5\n",
    "                if i.sum() > 0:\n",
    "                    FP_classes = torch.argmax(p_cls[~mask][i], 1)\n",
    "                    FPe = torch.bincount(FP_classes, minlength=self.nC).float().cpu()  # extra FPs\n",
    "\n",
    "            return loss, loss.item(), lx.item(), ly.item(), lw.item(), lh.item(), lconf.item(), lcls.item(), \\\n",
    "                   nT, TP, FP, FPe, FN, TC\n",
    "\n",
    "        else:\n",
    "            stride = self.img_dim / nG\n",
    "            p[..., 0] = torch.sigmoid(p[..., 0]) + self.grid_x  # x\n",
    "            p[..., 1] = torch.sigmoid(p[..., 1]) + self.grid_y  # y\n",
    "            p[..., 2] = torch.exp(p[..., 2]) * self.anchor_w  # width\n",
    "            p[..., 3] = torch.exp(p[..., 3]) * self.anchor_h  # height\n",
    "            p[..., 4] = torch.sigmoid(p[..., 4])  # p_conf\n",
    "            p[..., :4] *= stride\n",
    "\n",
    "            # reshape from [1, 3, 13, 13, 85] to [1, 507, 85]\n",
    "            return p.view(bs, self.nA * nG * nG, 5 + self.nC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darknet的前向传播解读（重要）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "  #每层加个打印就行了\n",
    "def forward(self, x, targets=None, batch_report=False, var=0):\n",
    "    self.losses = defaultdict(float)\n",
    "    is_training = targets is not None\n",
    "    layer_outputs = []\n",
    "    output = []\n",
    "\n",
    "    for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
    "        if module_def['type'] in ['convolutional', 'upsample', 'maxpool']:\n",
    "            #print(x.shape)\n",
    "\n",
    "            x = module(x)\n",
    "            print('conv/up: ',x.shape) \n",
    "        elif module_def['type'] == 'route':\n",
    "            layer_i = [int(x) for x in module_def['layers'].split(',')]\n",
    "            x = torch.cat([layer_outputs[i] for i in layer_i], 1)\n",
    "            print('route  : ',x.shape)      \n",
    "        elif module_def['type'] == 'shortcut':\n",
    "            layer_i = int(module_def['from'])\n",
    "            x = layer_outputs[-1] + layer_outputs[layer_i]\n",
    "            print('shortct: ',x.shape)    \n",
    "        elif module_def['type'] == 'yolo':\n",
    "            # Train phase: get loss\n",
    "            if is_training:\n",
    "                x, *losses = module[0](x, targets, batch_report, var)\n",
    "                for name, loss in zip(self.loss_names, losses):\n",
    "                    self.losses[name] += loss\n",
    "            # Test phase: Get detections\n",
    "            else:\n",
    "                x = module(x)\n",
    "                print('yolodec: ',x.shape)\n",
    "            output.append(x)\n",
    "        layer_outputs.append(x)\n",
    "    \n",
    "'''           \n",
    "每层的输出：\n",
    "torch.Size([1, 3, 416, 416])    #输入层原图（3*416*416），扩展了一个batch维度 \n",
    "#下面有107层输出\n",
    "conv/up:  torch.Size([1, 32, 416, 416])   #第一层卷积核32个，padding保持尺寸，输出的通道32\n",
    "conv/up:  torch.Size([1, 64, 208, 208])   #开始1/2降采样，strid=2,padding，64卷积核，此处输出通道64\n",
    "conv/up:  torch.Size([1, 32, 208, 208])   #1*1卷积核融合通道信息，32卷积核，channel=32\n",
    "conv/up:  torch.Size([1, 64, 208, 208])   #进行3*3+padding的same卷积通道延伸为64便于后面的残差连接\n",
    "shortct:  torch.Size([1, 64, 208, 208])   #shortcut连接，将其上第一行和第三行相加\n",
    "conv/up:  torch.Size([1, 128, 104, 104])  #略，都一样\n",
    "conv/up:  torch.Size([1, 64, 104, 104])\n",
    "conv/up:  torch.Size([1, 128, 104, 104])\n",
    "shortct:  torch.Size([1, 128, 104, 104])\n",
    "conv/up:  torch.Size([1, 64, 104, 104])\n",
    "conv/up:  torch.Size([1, 128, 104, 104])\n",
    "shortct:  torch.Size([1, 128, 104, 104])\n",
    "conv/up:  torch.Size([1, 256, 52, 52])\n",
    "conv/up:  torch.Size([1, 128, 52, 52])\n",
    "conv/up:  torch.Size([1, 256, 52, 52])\n",
    "shortct:  torch.Size([1, 256, 52, 52])\n",
    "conv/up:  torch.Size([1, 128, 52, 52])\n",
    "conv/up:  torch.Size([1, 256, 52, 52])\n",
    "shortct:  torch.Size([1, 256, 52, 52])\n",
    "conv/up:  torch.Size([1, 128, 52, 52])\n",
    "conv/up:  torch.Size([1, 256, 52, 52])\n",
    "shortct:  torch.Size([1, 256, 52, 52])\n",
    "conv/up:  torch.Size([1, 128, 52, 52])\n",
    "conv/up:  torch.Size([1, 256, 52, 52])\n",
    "shortct:  torch.Size([1, 256, 52, 52])\n",
    "conv/up:  torch.Size([1, 128, 52, 52])\n",
    "conv/up:  torch.Size([1, 256, 52, 52])\n",
    "shortct:  torch.Size([1, 256, 52, 52])\n",
    "conv/up:  torch.Size([1, 128, 52, 52])\n",
    "conv/up:  torch.Size([1, 256, 52, 52])\n",
    "shortct:  torch.Size([1, 256, 52, 52])\n",
    "conv/up:  torch.Size([1, 128, 52, 52])\n",
    "conv/up:  torch.Size([1, 256, 52, 52])\n",
    "shortct:  torch.Size([1, 256, 52, 52])\n",
    "conv/up:  torch.Size([1, 128, 52, 52])\n",
    "conv/up:  torch.Size([1, 256, 52, 52])\n",
    "shortct:  torch.Size([1, 256, 52, 52])\n",
    "conv/up:  torch.Size([1, 512, 26, 26])\n",
    "conv/up:  torch.Size([1, 256, 26, 26])\n",
    "conv/up:  torch.Size([1, 512, 26, 26])\n",
    "shortct:  torch.Size([1, 512, 26, 26])\n",
    "conv/up:  torch.Size([1, 256, 26, 26])\n",
    "conv/up:  torch.Size([1, 512, 26, 26])\n",
    "shortct:  torch.Size([1, 512, 26, 26])\n",
    "conv/up:  torch.Size([1, 256, 26, 26])\n",
    "conv/up:  torch.Size([1, 512, 26, 26])\n",
    "shortct:  torch.Size([1, 512, 26, 26])\n",
    "conv/up:  torch.Size([1, 256, 26, 26])\n",
    "conv/up:  torch.Size([1, 512, 26, 26])\n",
    "shortct:  torch.Size([1, 512, 26, 26])\n",
    "conv/up:  torch.Size([1, 256, 26, 26])\n",
    "conv/up:  torch.Size([1, 512, 26, 26])\n",
    "shortct:  torch.Size([1, 512, 26, 26])\n",
    "conv/up:  torch.Size([1, 256, 26, 26])\n",
    "conv/up:  torch.Size([1, 512, 26, 26])\n",
    "shortct:  torch.Size([1, 512, 26, 26])\n",
    "conv/up:  torch.Size([1, 256, 26, 26])\n",
    "conv/up:  torch.Size([1, 512, 26, 26])\n",
    "shortct:  torch.Size([1, 512, 26, 26])\n",
    "conv/up:  torch.Size([1, 256, 26, 26])\n",
    "conv/up:  torch.Size([1, 512, 26, 26])\n",
    "shortct:  torch.Size([1, 512, 26, 26])\n",
    "conv/up:  torch.Size([1, 1024, 13, 13])\n",
    "conv/up:  torch.Size([1, 512, 13, 13])\n",
    "conv/up:  torch.Size([1, 1024, 13, 13])\n",
    "shortct:  torch.Size([1, 1024, 13, 13])\n",
    "conv/up:  torch.Size([1, 512, 13, 13])\n",
    "conv/up:  torch.Size([1, 1024, 13, 13])\n",
    "shortct:  torch.Size([1, 1024, 13, 13])\n",
    "conv/up:  torch.Size([1, 512, 13, 13])\n",
    "conv/up:  torch.Size([1, 1024, 13, 13])\n",
    "shortct:  torch.Size([1, 1024, 13, 13])\n",
    "conv/up:  torch.Size([1, 512, 13, 13])\n",
    "conv/up:  torch.Size([1, 1024, 13, 13])\n",
    "shortct:  torch.Size([1, 1024, 13, 13])\n",
    "conv/up:  torch.Size([1, 512, 13, 13])\n",
    "conv/up:  torch.Size([1, 1024, 13, 13])\n",
    "conv/up:  torch.Size([1, 512, 13, 13])\n",
    "conv/up:  torch.Size([1, 1024, 13, 13])\n",
    "conv/up:  torch.Size([1, 512, 13, 13])\n",
    "conv/up:  torch.Size([1, 1024, 13, 13])\n",
    "conv/up:  torch.Size([1, 255, 13, 13])   #最大尺度的三个box信息得出（coco:3*（80+4+1）=255），可输出到检测YOLO层\n",
    "yolodec:  torch.Size([1, 507, 85])      #预测了507=13*13*3个box\n",
    "route  :  torch.Size([1, 512, 13, 13])  #理解了！单个route层存在的意义，为了便于使用序贯模型，见最上面route层的分析\n",
    "conv/up:  torch.Size([1, 256, 13, 13])\n",
    "conv/up:  torch.Size([1, 256, 26, 26])  #上采样层从13*13扩展 到26*26\n",
    "route  :  torch.Size([1, 768, 26, 26])  #26*26尺度的concat融合\n",
    "conv/up:  torch.Size([1, 256, 26, 26])\n",
    "conv/up:  torch.Size([1, 512, 26, 26])\n",
    "conv/up:  torch.Size([1, 256, 26, 26])\n",
    "conv/up:  torch.Size([1, 512, 26, 26])\n",
    "conv/up:  torch.Size([1, 256, 26, 26])\n",
    "conv/up:  torch.Size([1, 512, 26, 26])\n",
    "conv/up:  torch.Size([1, 255, 26, 26])  #中等尺度的输出，后接YOLO层检测\n",
    "yolodec:  torch.Size([1, 2028, 85])     #预测了2028=26*26*3个box\n",
    "route  :  torch.Size([1, 256, 26, 26])\n",
    "conv/up:  torch.Size([1, 128, 26, 26])  \n",
    "conv/up:  torch.Size([1, 128, 52, 52])  #对26*26上采样得到52*52特征图\n",
    "route  :  torch.Size([1, 384, 52, 52])  #concat融合之前的（52*52*256）得到52*52尺度的特征图\n",
    "conv/up:  torch.Size([1, 128, 52, 52])\n",
    "conv/up:  torch.Size([1, 256, 52, 52])\n",
    "conv/up:  torch.Size([1, 128, 52, 52])\n",
    "conv/up:  torch.Size([1, 256, 52, 52])\n",
    "conv/up:  torch.Size([1, 128, 52, 52])\n",
    "conv/up:  torch.Size([1, 256, 52, 52])\n",
    "conv/up:  torch.Size([1, 255, 52, 52])  #最小尺度特征图\n",
    "yolodec:  torch.Size([1, 8112, 85])     #8112=52*52*3个box\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
